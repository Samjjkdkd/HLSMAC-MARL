# --- Qatten specific parameters ---

# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 1.0
epsilon_finish: 0.05
epsilon_anneal_time: 50000

runner: "parallel"

batch_size: 128
buffer_size: 5000

# update the target network every {} episodes
target_update_interval: 200

# use the Q_Learner to train
agent_output_type: "q"
learner: "qatten_learner"
double_q: True
mixer: "qatten"
mixing_embed_dim: 32
hypernet_embed: 64
adv_hypernet_layers: 1
adv_hypernet_embed: 64


num_kernel: 4
is_minus_one: True
is_adv_attention: True
is_stop_gradient: True

n_head: 4  # attention head number
attend_reg_coef: 0.001  # attention regulation coefficient  # For MMM2 and 3s5z_vs_3s6z, it is 0.001
state_bias: True  # the constant value c(s) in the paper
mask_dead: False
weighted_head: False  # weighted head Q-values, for MMM2 and 3s5z_vs_3s6z, it is True
nonlinear: False  # non-linearity, for MMM2, it is True

# ResQ (Residual Q) parameters
use_resq: False  # Enable ResQ residual network
residual_embed_dim: 32  # Residual network embedding dimension
residual_layers: 2  # Number of layers in residual network
residual_weight_decay: 0.0  # Weight decay for residual network
consistency_loss_coef: 1.0  # Consistency loss coefficient
resq_init_scale: 0.1  # Initial scale for ResQ parameters

# TD(λ) parameters
lambda: 0.9  # TD(λ) lambda parameter, 0.0 means TD(0), 1.0 means Monte Carlo

# n step TD（优先，当n_step 不为1时，算法运行n step TD， 当n_step 为1时，算法运行TD(λ））
n_step: 1

burn_in_period: 100

name: "qatten"